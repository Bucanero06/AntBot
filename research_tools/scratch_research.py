
import glob
# Logging
import logging
import sys

import numpy as np
import pandas as pd

from research_tools import util
from research_tools.GenieLoaderForked import GenieLoader
from research_tools.labeling_algorythms import labeling, trend_scanning_labels
from research_tools.labeling_algorythms.filters import cusum_filter

logger = logging.getLogger(__name__)


def dataframe_to_series_list(input_dataframe: pd.DataFrame):
    return [s for _, s in input_dataframe.items()]


def volatility_triple_barrier_label_example(
        close_series: pd.Series,
        side_series: pd.Series, **tbl_kwargs):
    """
    This function is an example of how to use the triple barrier labeling algorythm

    This function takes in a series of closing prices and a labeled series generated by the example_arb_strategy
    function as input. It applies a triple barrier labeling algorithm to the input data to generate a
    meta-labeled series, which is a series of labeled events (0, 1, or -1 & 0 or 1) indicating the success or failure
    of a trade for the primary and for the secondary model. The function returns a DataFrame containing the input data and the meta-labeled series.

    """

    assert close_series.index.equals(side_series.index)

    '''Primary Model'''
    # Combine the side series with the close series
    side_labeled_ohlcv_data = pd.concat([close_series, side_series], axis=1, keys=['close', 'side'])

    assert side_labeled_ohlcv_data.index.equals(close_series.index)

    # Remove Look ahead biase by lagging the signal
    side_labeled_ohlcv_data['side'] = side_labeled_ohlcv_data['side'].shift(1)  # Needed for the labeling algorythm

    # Save the raw data
    raw_data = side_labeled_ohlcv_data.copy()

    # Drop the NaN values from our data set
    side_labeled_ohlcv_data = side_labeled_ohlcv_data.dropna(axis=0, how='any', inplace=False)

    # Make sure side is an integer
    side_labeled_ohlcv_data['side'] = side_labeled_ohlcv_data['side'].astype(int)

    # Compute daily volatility
    daily_vol = util.get_daily_vol(close=side_labeled_ohlcv_data['close'], lookback=50)
    logger.info(f'{daily_vol = }')

    # Apply Symmetric CUSUM Filter and get timestamps for events
    # Note: Only the CUSUM filter needs a point estimate for volatility
    cusum_events = cusum_filter(side_labeled_ohlcv_data['close'], threshold=daily_vol.mean() * 0.5)
    logger.info(f'{cusum_events = }')
    if len(cusum_events) == 0:
        logger.warning('\nNo CUSUM events found with the given parameters and provided data')
        return None

    # Compute vertical barrier
    vertical_barriers = labeling.add_vertical_barrier(t_events=cusum_events, close=side_labeled_ohlcv_data['close'],
                                                      # num_days=1, num_hours=0, num_minutes=0, num_seconds=0
                                                      num_days=tbl_kwargs.get('vertical_barrier_num_days', 1),
                                                      num_hours=tbl_kwargs.get('vertical_barrier_num_hours', 0),
                                                      num_minutes=tbl_kwargs.get('vertical_barrier_num_minutes', 0),
                                                      num_seconds=tbl_kwargs.get('vertical_barrier_num_seconds', 0)
                                                      )
    logger.info(f'{vertical_barriers = }')
    triple_barrier_events = labeling.get_events(close=side_labeled_ohlcv_data['close'],
                                                side_prediction=side_labeled_ohlcv_data['side'],
                                                t_events=cusum_events,
                                                pt_sl=tbl_kwargs.get('pt_sl', [1, 1]),
                                                target=daily_vol,
                                                vertical_barrier_times=vertical_barriers,
                                                min_ret=tbl_kwargs.get('min_ret', 0.001),
                                                num_threads=tbl_kwargs.get('num_threads', 1)
                                                )

    labels = labeling.get_bins(triple_barrier_events, side_labeled_ohlcv_data['close'])

    # Shift the side series back to the original position
    raw_data.dropna(axis=0, how='any', inplace=True)
    raw_data['side'] = raw_data['side'].astype(int)

    # Change raw_data side name to direction
    raw_data.rename(columns={'side': 'direction'}, inplace=True)

    labels.columns = ["ret", "target_ret", "meta_target", "prim_target"]

    returning_df = pd.concat([raw_data, labels], axis=1).fillna(0)

    # Add the first row since they were removed in the beginning after the lagging. Include dates
    first_row = {'close': close_series[0], 'direction': side_series[0], 'ret': 0, 'target_ret': 0, 'meta_target': 0,
                 'prim_target': 0}
    first_row = pd.DataFrame(first_row, index=[close_series.index[0]])
    returning_df = pd.concat([first_row, returning_df], axis=0)  # if memory is an issue switch to dask or loop-append

    # Change prim_target and meta_target to int
    returning_df['prim_target'] = returning_df['prim_target'].astype(int)
    returning_df['meta_target'] = returning_df['meta_target'].astype(int)

    logger.info("TBL-ML-Data")
    logger.info(returning_df[["direction", "prim_target", "meta_target"]].head(10))

    # Print unique values
    unique_values = returning_df[["direction", "prim_target", "meta_target"]].apply(lambda x: x.unique())
    logger.info("Unique Values")
    logger.info(unique_values)

    return returning_df


def save_output_data(output_data, output_file_dir, output_file_name, output_file_type):
    if isinstance(output_file_type, list):
        # This will be a self recursive function
        # assert all the elements in the list are strings
        assert all([isinstance(file_type, str) for file_type in output_file_type])
        for file_type in output_file_type:
            save_output_data(output_data, output_file_dir, output_file_name, file_type)
        return

    if "csv" in output_file_type:
        output_data.to_csv(f"{output_file_dir}/{output_file_name}.csv")
    if "pkl" in output_file_type:
        output_data.to_pickle(f"{output_file_dir}/{output_file_name}.pkl")


def flexible_tbm_labeling(
        close_series: pd.Series,
        instrument_name: str, **tbl_kwargs: dict):
    side_series = trend_scanning_labels(price_series=close_series, t_events=close_series.index,
                                        look_forward_window=20,
                                        min_sample_length=5, step=1)["bin"]

    assert close_series.index.equals(side_series.index), "close_series and side_series must have the same index"

    triple_barrier_labeled_data = volatility_triple_barrier_label_example(
        close_series=close_series,
        side_series=side_series, **tbl_kwargs)

    if triple_barrier_labeled_data is None:
        logging.warning("flexible_tbm_labeling returning a None value, most likely no CUSUM events were found")
        return None

    if tbl_kwargs.get('save_output', False):
        # Save individual data
        instrument_output_file_dir = f'{instrument_name}_{tbl_kwargs.get("output_file_name", None)}'
        logging.info(f"Saving {instrument_name} data individually to {instrument_output_file_dir}")
        save_output_data(triple_barrier_labeled_data,
                         output_file_dir=tbl_kwargs.get('output_file_dir', None),
                         # output_file_name=tbl_kwargs.get('output_file_name', None),
                         output_file_name=instrument_output_file_dir,
                         output_file_type=tbl_kwargs.get('output_file_type', None))

    return triple_barrier_labeled_data


def TBM_labeling(
        input_data_params: dict,
        tbl_params: dict,
        output_data_params: dict,
        **kwargs
):
    """The main script sets up input and output parameters and uses the GenieLoader module to load the input data from
    a CSV file. It then applies the example_arb_strategy (arbitrary) function to the input data to generate a labeled series.
    The labeled series and the closing price series are then passed to the daily_vol_triple_barrier_label_example
    function to generate a meta-labeled DataFrame. Finally, the script saves the meta-labeled DataFrame to an output
    file using the save_output_data function."""

    # todo should also be accepted as a parameter or at least pick up from a checkpoint like side labeled data

    # Load data and convert to a VBT compatible format for vectorized backtesting e.g. during the side labeling
    genie_loader = GenieLoader()

    from os import path
    pickle_file_path = input_data_params.get("pickle_file_path", False)
    if not path.isfile(pickle_file_path):
        symbols_data_obj = genie_loader.fetch_data(**input_data_params)
        if pickle_file_path:
            symbols_data_obj.save(pickle_file_path)
    else:
        symbols_data_obj = genie_loader.load_vbt_pickle(pickle_file_path)

    '''Label the sides of the entries when the decision is made, not when the trade is opened -1 0 1 labeled series'''
    # side_df = example_arb_strategy(VBT_OHLCV_Data=symbols_data_obj)  # this if a strategy using VBT is used ...
    # logger.info(f'{side_df_.head(10) = }')
    # todo this needs to be passed in rather than hard coded
    # side_df = trend_scanning_labels(price_series=symbols_data_obj.close, t_events=symbols_data_obj.close.index, look_forward_window=20,
    #                                 min_sample_length=5, step=1)["bin"]

    '''Compute features from the vbt OHLCV data'''  # FIXME can be moved to another process
    # features = compute_features_from_vbt_data(symbols_data_obj) # FIXME

    """<<<<<<<<<<<<<<                          TRIPLE BARRIER Meta LABELING                            >>>>>>>>>>>>"""

    # fixme still using vbt data model
    open_df = pd.DataFrame(symbols_data_obj.open)
    high_df = pd.DataFrame(symbols_data_obj.high)
    low_df = pd.DataFrame(symbols_data_obj.low)
    close_df = pd.DataFrame(symbols_data_obj.close)
    volume_df = pd.DataFrame(symbols_data_obj.volume)

    # Convert
    '''Using the close and side series and other tbl configuration parameters, label --> triple barrier labeling'''




    tbml_data = flexible_tbm_labeling(
        # open_series: pd.Series,
    #         high_series: pd.Series,
    #         low_series: pd.Series,
    #         close_series: pd.Series,
    #         volume_series: pd.Series,
    #         instrument_name: str, **tbl_kwargs: dict
    )

    return tbml_data


if __name__ == "__main__":
    ROOT_DATA_DIR = "/home/ruben/PycharmProjects/Genie-Trader/Data/raw_data"
    DATA_FILE_DIRS = glob.glob(f"{ROOT_DATA_DIR}/**", recursive=True)
    DATA_FILE_NAMES = [
        "XAUUSD_GMT+0_NO-DST_M1.csv",
        # "US_Brent_Crude_Oil_GMT+0_NO-DST_M1.csv",
    ]
    OUTPUT_FILE_DIR = "/home/ruben/PycharmProjects/Genie-Trader/dev_studies_workdir"
    OUTPUT_FILE_NAME = "tbml_data"

    INPUT_DATA_PARAMS = dict(
        # todo this will be changed as needed for the endpoint to save the data e.g. S3-bucket
        # todo maybe i would like to also allow for data to be loaded from an exchange api
        data_file_dirs=DATA_FILE_DIRS,
        data_file_names=DATA_FILE_NAMES,
        rename_columns={"Open": "open", "High": "high", "Low": "low", "Close": "close", "Tick volume": "volume"},
        # rename_columns={"Open": "open", "High": "high", "Low": "low", "Close": "close"},
        scheduler='threads',
        first_or_last='first',
        # n_rows=10000,
        #
        # pickle_file_path=f"data_temp_brent.pkl",  # if exists, then load from pickle file instead, else will create it
        # pickle_file_path=f"data_temp_xau.pkl",  # if exists, then load from pickle file instead, else will create it
        pickle_file_path=f"data_temp.pkl",  # if exists, then load from pickle file instead, else will create it

    )
    TBL_PARAMS = dict(
        pt_sl=[0.01, 0.01],
        min_ret=0.001,  # todo allow user to pass a str of a function to be applied to the data to calculate this
        num_threads=28,
        #
        #  Number of D/H/m/s to add for vertical barrier
        vertical_barrier_num_days=0,
        vertical_barrier_num_hours=0,
        vertical_barrier_num_minutes=5,
        vertical_barrier_num_seconds=0,
    )
    OUTPUT_DATA_PARAMS = dict(
        # todo this will be changed as needed for the endpoint to save the data e.g. S3-bucket
        output_file_dir=OUTPUT_FILE_DIR,
        output_file_name=OUTPUT_FILE_NAME,
        save_output=True,
        output_file_type=[
            "csv",
            # "pkl"
        ],  # can be csv, pickle, vbt, or list of these
    )

    # %%%%%%%%%%%%--START-EXECUTION--%%%%%%%%%%%%
    TBM_labeling(
        input_data_params=INPUT_DATA_PARAMS,
        tbl_params=TBL_PARAMS,
        output_data_params=OUTPUT_DATA_PARAMS,
    )
    # %%%%%%%%%%%--END-OF-EXECUTION--%%%%%%%%%%%
